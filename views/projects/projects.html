<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Projects and Achievements</title>
    <link rel="stylesheet" href="/font.css" />
    <link rel="stylesheet" href="/styles.css" />
    <link rel="stylesheet" href="/views/projects/projects.css" />
    <style type="text/css">
      body {
        background: url("/assets/svg/projects-bg/projects-bg.svg") no-repeat;
        background-size: cover;
        background-attachment: fixed;
      }
    </style>
    <link
      href="https://fonts.googleapis.com/css2?family=K2D:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap"
      rel="stylesheet"
    />
    <script type="module">
      import { importHtml } from "/core/framework/index.js";
      import { elementProcessor } from "/core/framework/index.js";

      importHtml({
        path: "/components/navbar/navbar.html",
        elementId: "importedHeader",
        document: document,
      });
    </script>
  </head>
  <body>
    <div id="importedHeader"></div>
    <main class="projects-main">
      <h1 class="k2d-semibold">Projects and Achievements</h1>
      <div class="project">
        <h1 class="k2d-semibold">Project 1: Advanced Web Scraper</h1>
        <h2 class="k2d-semibold">Description:</h2>
        <p>
          The Advanced Web Scraper is a highly sophisticated tool designed to
          extract large volumes of data from various websites efficiently. This
          scraper is built to handle dynamic content, manage CAPTCHA challenges,
          and organize the scraped data into a structured format for analysis
          and use in different applications.
        </p>

        <img
          src="/assets/img/webscraper.png"
          alt="The image is a diagram representing a system architecture for processing web content. Here’s a detailed description:
              1.	API Entry Points:
              •	On the left, a box labeled “Any API entry point” lists possible entry points: Webapp, Desktop, and Mobile. These are written in blue text, indicating hyperlinks.
              2.	CLI:
              •	The API entry points lead to a box labeled “CLI,” which connects with several other components.
              3.	HTML Parser:
              •	The “CLI” connects to an “HTML parser,” which interacts with several components in the system.
              4.	Graph Resolver:
              •	A “Graph Resolver” module is linked to the HTML parser and is responsible for resolving graph cycling.
              5.	URL Extractor:
              •	The “HTML parser” also connects to a “URL Extractor,” which is tasked with disassembling pages into building blocks and extracting URLs from web pages.
              6.	Databases:
              •	The URL Extractor outputs to a “NoSQL Document DB.”
              •	A “SQL/NoSQL? URL Queue” connects back to the HTML parser, indicating a decision or question regarding the type of database to use for the URL queue.
              7.	Additional Information:
              •	Below the main diagram, there is an area with handwritten notes labeled “Service,” “Database,” and “Cache,” each in different colors.
              •	There is a “P.S.” note at the bottom-right, mentioning that services are intended to be modules following the ISO/IEC/IEEE 24765:2010 standard, relating to systems and software engineering vocabulary."
          class="diagram"
        />

        <h3 class="k2d-semibold">Key Features:</h3>
        <ul>
          <li>
            Capable of scraping data from both static and dynamic web pages.
          </li>
          <li>
            Utilizes headless browsers like Puppeteer to handle JavaScript-heavy
            sites.
          </li>
          <li>
            Integrates CAPTCHA-solving services to bypass anti-scraping
            measures.
          </li>
          <li>
            Stores data in a well-structured format, such as CSV or JSON, for
            easy analysis.
          </li>
          <li>
            Provides scheduling functionality to automate scraping tasks at
            regular intervals.
          </li>
        </ul>

        <h3 class="k2d-semibold">Technologies Used:</h3>
        <ul>
          <li>Programming Languages: Python, JavaScript</li>
          <li>Libraries and Tools: BeautifulSoup, Scrapy, Puppeteer</li>
          <li>Data Storage: MongoDB, MySQL</li>
          <li>Automation: Cron Jobs for task scheduling</li>
        </ul>

        <h3 class="k2d-semibold">Challenges and Solutions:</h3>
        <ul>
          <li>
            <strong>Challenge:</strong> Handling dynamic content generated by
            JavaScript.<br />
            <strong>Solution:</strong> Used Puppeteer to control a headless
            browser, allowing the scraper to wait for and interact with
            JavaScript-generated content.
          </li>
          <li>
            <strong>Challenge:</strong> Bypassing CAPTCHA challenges.<br />
            <strong>Solution:</strong> Integrated third-party CAPTCHA-solving
            services and implemented machine learning models to automatically
            solve CAPTCHA challenges.
          </li>
          <li>
            <strong>Challenge:</strong> Ensuring data consistency and avoiding
            IP bans.<br />
            <strong>Solution:</strong> Implemented proxy rotation and request
            throttling to mimic human browsing behavior and avoid detection.
          </li>
        </ul>
      </div>

      <h1 class="k2d-semibold">
        Project 2: Machine Learning Pipeline Refactoring
      </h1>
      <div class="project">
        <h2 class="k2d-semibold">Description:</h2>
        <p>
          This project involved refactoring a machine learning pipeline used for
          predictive analytics in a financial application. The goal was to
          improve code readability, performance, and maintainability, as well as
          to update documentation and fix existing shell scripts.
        </p>

        <img
          src="/assets/img/querymodel.jpg"
          alt="	1.	Title:
              •	The title at the top reads “Querying the model.”
              2.	User Query:
              •	At the center top of the diagram is a box labeled “User Query,” representing the initial input from the user.
              3.	Named Entity Recognition:
              •	To the left, the user query is passed into a “Named Entity Recognition” (NER) module, which identifies key entities in the query.
              4.	Embedding Model:
              •	The user query is also sent to an “Embedding Model,” which transforms the query into a vector representation.
              5.	Vector Database:
              •	The vector output from the embedding model is used to interact with a “Vector Database” at the bottom left of the diagram.
              6.	Filter and Filtered Database:
              •	The vector database connects to a “Filter” module, which then outputs to a “Filtered Database.”
              7.	Context Results:
              •	The filtered database provides “Context Results,” which are sent to a “Prompt” module.
              8.	Prompt:
              •	The prompt module is responsible for forming a query for the LLM (Large Language Model). It instructs, “Answer the Query. Only use Context to construct the answer.”
              9.	LLM and UI:
              •	The prompt is sent to the “LLM” (Large Language Model), which generates an answer. The answer is then sent to the “UI” (User Interface), where it is presented to the user."
          class="diagram"
        />

        <h3 class="k2d-semibold">Key Features:</h3>
        <ul>
          <li>
            Refactored the codebase for better modularity and readability.
          </li>
          <li>
            Improved the performance of data processing and model training
            stages.
          </li>
          <li>
            Updated and expanded documentation for better clarity and ease of
            use.
          </li>
          <li>Fixed and optimized shell scripts used for automation tasks.</li>
        </ul>

        <h3 class="k2d-semibold">Technologies Used:</h3>
        <ul>
          <li>Programming Languages: Python, Shell Scripting</li>
          <li>Machine Learning: scikit-learn, pandas</li>
          <li>Documentation: Sphinx, Markdown</li>
          <li>Version Control: Git</li>
        </ul>

        <h3 class="k2d-semibold">Challenges and Solutions:</h3>
        <ul>
          <li>
            <strong>Challenge:</strong> Ensuring backward compatibility while
            refactoring.<br />
            <strong>Solution:</strong> Wrote extensive tests and used version
            control to manage changes, ensuring that the refactored code
            produced the same results as the original.
          </li>
          <li>
            <strong>Challenge:</strong> Improving performance without
            sacrificing accuracy.<br />
            <strong>Solution:</strong> Profiled the code to identify bottlenecks
            and implemented efficient algorithms and data structures to enhance
            performance.
          </li>
        </ul>
      </div>
    </main>
  </body>
</html>
